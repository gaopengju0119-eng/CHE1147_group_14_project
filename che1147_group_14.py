# -*- coding: utf-8 -*-
"""CHE1147_Group_14

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JpOU4Ce4eg8XFI477AFu9eXqG6d6VAQJ
"""

# ==========================================
# CHE1147 Project: Heat Capacity Prediction on QM9
# Group 14, Members: Matt Smrke, Pengju Gao, Sebastian Cook
# ==========================================

# ==========================================
# 1. ENVIRONMENT SETUP & IMPORTS
# ==========================================

# --- System & Configuration ---
import os
import warnings

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
from tqdm.auto import tqdm

# Configure warnings and plotting style
warnings.filterwarnings('ignore')
sns.set(style='whitegrid')
pd.set_option('display.max_columns', None)

# --- PyTorch & PyG Dependency Installation (Dynamic) ---
import torch
def format_pytorch_version(version):
    return version.split('+')[0]

TORCH_VERSION = format_pytorch_version(torch.__version__)
CUDA_VERSION = torch.version.cuda.replace('.', '') if torch.cuda.is_available() else 'cpu'

print(f"Detected PyTorch: {TORCH_VERSION} | CUDA: {CUDA_VERSION}")
print("Installing PyTorch Geometric dependencies (this may take a minute)...")

# --- Library Imports ---

# 1. Deep Learning (Graph)
import torch_geometric
from torch_geometric.datasets import QM9
from torch_geometric.data import Data

# 2. Chemistry (RDKit)
from rdkit import Chem
from rdkit.Chem import Descriptors, AllChem

# 3. Scikit-Learn (Preprocessing & Selection)
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, KFold, GridSearchCV
from sklearn.feature_selection import VarianceThreshold

# 4. Scikit-Learn (Models)
from sklearn.neighbors import KNeighborsRegressor
from sklearn.linear_model import RidgeCV, LassoCV, Lasso
from sklearn.ensemble import RandomForestRegressor

# 5. Advanced Modeling (XGBoost)
import xgboost as xgb

# 6. Interpretability & Metrics
import shap
from sklearn.metrics import r2_score, mean_absolute_error
from sklearn.utils import resample
from sklearn.base import clone

print("\n‚úÖ All imports and installations complete. Ready to proceed!")

# ==========================================
# 2. LOAD DATASET (QM9)
# ==========================================

# Define the path where the dataset will be stored
# We use os.path.join to ensure cross-platform compatibility
dataset_path = os.path.join('.', 'data', 'QM9')

print(f"Loading QM9 dataset from: {dataset_path}")

# Initialize the QM9 dataset
# This will automatically download the data if it's not present
dataset = QM9(root=dataset_path)

# --- Basic Dataset Inspection ---
# Always verify the shape of your data immediately after loading (EDA Best Practice)
print(f"\n‚úÖ Dataset Loaded Successfully!")
print(f"Dataset Name: {dataset.name}")
print(f"Total Molecules: {len(dataset)}")
print(f"Number of Features per Atom: {dataset.num_features}")
print(f"Number of Target Properties: {dataset.num_classes}")

# Display the first molecule to understand the data structure
print("\n--- Example: First Molecule ---")
data_point = dataset[0]
print(data_point)
print(f" - x (Atom Features): {data_point.x.shape}")
print(f" - edge_index (Connectivity): {data_point.edge_index.shape}")
print(f" - y (Target Properties): {data_point.y.shape}")

# ==========================================
# 3. INSPECT DATASET (EDA Sanity Check)
# ==========================================
# This cell checks if the dataset has been imported successfully and inspects its structure.

# --- 1. Verify Dataset Size ---
# The "shape" of the dataset is its total length (number of molecules).
print(f"--- Dataset Shape ---")
print(f"Total number of molecules (rows) in the dataset: {len(dataset)}")

# --- 2. Inspect the First 5 Rows (Molecules) ---
# As per EDA guidelines, inspecting raw samples helps identify data structure.
print(f"\n--- Showing the 'Form' of the First 5 Rows ---")

num_rows_to_show = 5

for i in range(num_rows_to_show):
    # Get the data point (molecule) at index i
    data_point = dataset[i]

    # Print the summary 'form' of the Data object.
    # This shows the dimensionality of key tensors:
    #   x:          Node features (Atom types, etc.)
    #   edge_index: Graph connectivity (Bonds)
    #   pos:        3D coordinates of atoms
    #   y:          Target labels (19 properties, including Heat Capacity)
    print(f"\nRow {i}:")
    print(data_point)

# ==========================================
# 4. DATA TRANSFORMATION & FEATURE ENGINEERING
# ==========================================
# This cell converts the Graph (PyG) data into a Tabular (Pandas) format.
# It also performs Feature Engineering by creating new descriptors.

# --- 1. Setup & Constants ---
# (Libraries already imported in Cell 1, but listed here for clarity)
# import pandas as pd
# from rdkit import Chem, Descriptors
# from tqdm.auto import tqdm

# Map QM9 edge attributes (0-3) to RDKit BondTypes
BOND_TYPE_MAP = {
    0: Chem.BondType.SINGLE,
    1: Chem.BondType.DOUBLE,
    2: Chem.BondType.TRIPLE,
    3: Chem.BondType.AROMATIC,
}

# Target properties in QM9 (Cv is index 11)
QM9_TARGET_NAMES = [
    'mu', 'alpha', 'homo', 'lumo', 'gap', 'r2', 'zpve',
    'U0', 'U', 'H', 'G', 'Cv', 'U0_atom', 'U_atom',
    'H_atom', 'G_atom', 'A', 'B', 'C'
]

# --- 2. Helper Function: Graph to Molecule ---
def get_mol_from_pyg_data(data):
    """
    Converts a PyG 'Data' object into an RDKit 'Mol' object and SMILES.
    Handles atom mapping and bond reconstruction.
    """
    mol = Chem.RWMol()

    # A. Add Atoms (Nodes)
    atomic_nums = data.z.tolist()
    for atom_num in atomic_nums:
        mol.AddAtom(Chem.Atom(int(atom_num)))

    # B. Add Bonds (Edges)
    rows, cols = data.edge_index.tolist()
    bond_type_indices = data.edge_attr.argmax(dim=1).tolist()

    # Use a set to avoid adding the same bond twice (since graphs are undirected)
    added_bonds = set()

    for i, j, b_idx in zip(rows, cols, bond_type_indices):
        if i < j:
            bond_type = BOND_TYPE_MAP[b_idx]
            mol.AddBond(i, j, bond_type)
            added_bonds.add((i, j))

    # C. Sanitize & Canonicalize
    try:
        smiles = Chem.MolToSmiles(mol)
        clean_mol = Chem.MolFromSmiles(smiles) # Robust check
        return clean_mol, smiles
    except Exception:
        return None, None

# --- 3. Main Processing Loop ---
all_molecules_data = []
skipped_count = 0

print(f"Processing {len(dataset)} molecules for Feature Extraction...")

for data in tqdm(dataset, desc="Converting & Engineering"):
    try:
        # Data Cleaning: Skip invalid entries
        if data.y is None or data.y.shape[0] == 0:
            skipped_count += 1
            continue

        mol, smiles = get_mol_from_pyg_data(data)

        if mol is None or smiles is None:
            skipped_count += 1
            continue

        # Feature Engineering
        # Domain knowledge: larger molecules (MW) often have higher Heat Capacity.
        molecular_weight = Descriptors.MolWt(mol)
        atom_count = data.num_nodes
        bond_count = int(data.num_edges / 2)

        # Construct Row
        mol_data = {
            'CanonicalSMILES': smiles,
            'MolecularWeight': molecular_weight,
            'AtomCount': atom_count,
            'BondCount': bond_count,
        }

        # Add Targets
        target_properties = data.y.tolist()[0]
        for name, value in zip(QM9_TARGET_NAMES, target_properties):
            mol_data[name] = value

        all_molecules_data.append(mol_data)

    except Exception as e:
        skipped_count += 1

# --- 4. Final DataFrame Creation ---
df_qm9_tabular = pd.DataFrame(all_molecules_data)

print(f"\n‚úÖ Processing Complete!")
print(f" - Successfully Converted: {len(df_qm9_tabular)}")
print(f" - Skipped (Errors/Missing): {skipped_count}")
print(f" - Final Shape: {df_qm9_tabular.shape}")

# ==========================================
# 5. DATA CLEANING (Remove Duplicates)
# ==========================================
# This cell removes duplicate molecules to ensure dataset integrity.

# --- 1. Initial Inspection ---
# Always log the shape before modification to track data loss.
print(f"Original DataFrame shape (before): {df_qm9_tabular.shape}")

# --- 2. Drop Duplicates Based on Canonical SMILES ---
# Why 'CanonicalSMILES'?
# Because RDKit normalizes the string, we can be sure that
# "CCO" and "OCC" (ethanol) are treated as the exact same string.
#
# Parameters:
# - subset=['CanonicalSMILES']: Check only this column for identity.
# - keep='first': Retain the first occurrence, discard the rest.
df_qm9_cleaned = df_qm9_tabular.drop_duplicates(subset=['CanonicalSMILES'], keep='first')

# Reset the index to avoid gaps in the index numbers (good practice)
df_qm9_cleaned = df_qm9_cleaned.reset_index(drop=True)

# --- 3. Final Inspection & Reporting ---
print(f"Cleaned DataFrame shape (after):   {df_qm9_cleaned.shape}")

# Calculate impact
num_removed = len(df_qm9_tabular) - len(df_qm9_cleaned)
print(f"Total duplicate rows removed:      {num_removed}")
print(f"Percentage of data retained:       {len(df_qm9_cleaned)/len(df_qm9_tabular)*100:.2f}%")

# ==========================================
# 6. DEFINE FEATURES (X) AND TARGET (y)
# ==========================================
# This cell establishes the Supervised Learning problem structure: y = f(X).

# --- 1. Define the Target (y) ---
# The objective is to predict Heat Capacity ('Cv').
target_column = 'Cv'
y = df_qm9_cleaned[target_column]

print(f"Target 'y' ({target_column}) successfully created. Shape: {y.shape}")

# --- 2. Define the Features (X) ---
# We create the Feature Matrix X by dropping:
# 1. The Target ('Cv') -> To prevent Data Leakage.
# 2. 'CanonicalSMILES' -> Non-numeric string identifier (not usable by regression models).
#
# The remaining 21 features include:
# - 3 Engineered Descriptors: MolecularWeight, AtomCount, BondCount.
# - 18 Physicochemical Properties: e.g., HOMO, LUMO, Dipole Moment.
#   (Using known physical properties to predict unknown ones is a standard
#    approach in QSPR/QSAR modeling).
X = df_qm9_cleaned.drop(columns=[target_column, 'CanonicalSMILES'])

print(f"Features 'X' successfully created. Shape: {X.shape}")

# --- 3. Feature Space Inspection ---
print(f"\n--- {len(X.columns)} Features (X) Selected for Modeling ---")
print(X.columns.tolist())

# --- 4. Data Verification ---
# Ensure X is purely numeric before moving to Scaling.
print("\n--- Sample Features (X: First 5 Rows) ---")
print(X.head())

print(f"\n--- Sample Target (y: First 5 Values) ---")
print(y.head())

# ==========================================
# 7. EXPLORATORY DATA ANALYSIS (EDA)
# ==========================================
# This cell analyzes the statistical properties of the Target (y) and Features (X).

# --- 1. Target Variable Analysis ('Cv') ---
print("--- Target (y = 'Cv') Statistical Summary ---")
# We check the mean and std dev to understand the baseline prediction error.
print(y.describe())

print("\n--- Plotting Target Distribution ---")
plt.figure(figsize=(8, 5))
# A histogram helps us verify if the target is normally distributed (Gaussian).
# If it were highly skewed, we might need a log-transform
y.hist(bins=50, edgecolor='black', color='skyblue')
plt.title("Distribution of Heat Capacity (Cv)")
plt.xlabel("Heat Capacity (Cv) [cal/mol¬∑K]")
plt.ylabel("Frequency (Count)")
plt.grid(False) # Clean look
plt.show()

# --- 2. Feature Space Analysis ---
print("--- Features (X) Statistical Summary ---")
# CRITICAL OBSERVATION:
# Look at the 'mean' and 'max' rows below.
# You will see massive differences (e.g., MolecularWeight ~100 vs HOMO ~ -0.2).
# This statistical evidence PROVES the need for StandardScaler in the next step.
print(X.describe())

print("\n--- Plotting Feature Distributions ---")
# Visualizing feature spreads helps identify outliers and data shape.
# 'layout=(-1, 4)' arranges the 21 subplots automatically into 4 columns.
X.hist(bins=25, figsize=(16, 20), edgecolor='black', layout=(-1, 4), color='lightgray')

plt.suptitle("Distributions of All 21 Features", y=1.01, fontsize=16)
plt.tight_layout() # Adjusts spacing to prevent label overlap
plt.show()

# ==========================================
# 8. DATA SPLITTING (Train/Test Split)
# ==========================================
# This cell separates the data into Training (80%) and Testing (20%) sets.

# --- 1. Define Splitting Parameters ---
# We set a random_state to ensure Reproducibility.
# This guarantees that every time you run this code, the specific molecules
# in the 'train' vs 'test' sets remain exactly the same.
TEST_SIZE = 0.20   # 20% held out for final evaluation
RANDOM_STATE = 42  # The standard seed for reproducible science

# --- 2. Perform the Split ---
# We randomly split the data to measure Generalization Error later.
# The Test set acts as "Unseen Data".
# (train_test_split was imported in Cell 1)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE
)

# --- 3. Verification ---
# Confirming shapes ensures no data was lost during the operation.
print("--- Data Splitting Complete ---")
print(f"Original Data: {X.shape[0]} samples")
print("-" * 30)
print(f"Training Set:  {X_train.shape[0]} samples ({100*(1-TEST_SIZE):.0f}%) -> Used for Model Fitting")
print(f"Testing Set:   {X_test.shape[0]} samples ({100*TEST_SIZE:.0f}%) -> Used for Final Evaluation")

# ==========================================
# 9. FEATURE SCALING (Standardization)
# ==========================================
# This cell standardizes features to have zero mean and unit variance.

# --- 1. Initialize the Scaler ---
# We use StandardScaler (Z-score normalization).
scaler = StandardScaler()

# --- 2. Fit the Scaler *ONLY* on Training Data ---
# We .fit() ONLY on X_train to avoid "Data Leakage".
# If we fit on the whole dataset, the model would "peek" at the test set's
# distribution, invalidating our results.
print("Fitting scaler on training data (X_train)...")
scaler.fit(X_train)

# --- 3. Transform Both Train and Test Data ---
# We apply the learned mean/std from Train to both sets.
print("Applying scaling to both train and test sets...")

# Convert output back to DataFrame to preserve column names for analysis
X_train_scaled = pd.DataFrame(
    scaler.transform(X_train),
    columns=X_train.columns,
    index=X_train.index
)

X_test_scaled = pd.DataFrame(
    scaler.transform(X_test),
    columns=X_test.columns,
    index=X_test.index
)

# --- 4. Verification ---
print("\n--- Scaled Training Data (First 5 Rows) ---")
print(X_train_scaled.head())

print("\n--- Verification of Statistics ---")
# Check if scaling worked:
# Mean should be approx 0.0 (e.g., 1.2e-16)
# Std Dev should be exactly 1.0
print(X_train_scaled.describe().loc[['mean', 'std']])

# ==========================================
# 10. FEATURE-TARGET CORRELATION ANALYSIS
# ==========================================
# This cell analyzes how each feature relates to the Target (Cv) and to other features.

# --- 1. Prepare Data for Correlation ---
# CRITICAL: We perform this ONLY on Training Data.
# Looking at Test Data correlations here would be "peeking" (Data Leakage).
train_data_for_corr = pd.concat(
    [X_train_scaled.reset_index(drop=True), y_train.reset_index(drop=True)],
    axis=1
)

# --- 2. Calculate Pearson Correlation ---
# A value of +1.0 = Perfect Positive Linear Relationship
# A value of -1.0 = Perfect Negative Linear Relationship
# A value of  0.0 = No Linear Relationship
corr_matrix = train_data_for_corr.corr()

# --- 3. Visualization: Annotated Heatmap ---
print("--- Plotting Feature-Target Correlation Heatmap ---")

plt.figure(figsize=(24, 20)) # Large size to accommodate 22x22 grid
sns.heatmap(
    corr_matrix,
    annot=True,             # Show the correlation coefficient in the cell
    fmt='.2f',              # Round to 2 decimal places for readability
    annot_kws={"size": 9},  # Font size for the numbers
    cmap='coolwarm',        # Red = Positive, Blue = Negative
    center=0,               # Center the colors at 0.0 (white)
    linewidths=0.5,         # Add lines between cells for clarity
    cbar_kws={"shrink": 0.8} # Shrink the color bar slightly
)

plt.title("Correlation Matrix: Features (X) vs Target (Cv)", fontsize=18, pad=20)
plt.xticks(rotation=45, ha='right') # Rotate labels for better fit
plt.yticks(rotation=0)
plt.tight_layout()
plt.show()

# --- 4. Quantitative Analysis: Target Correlations ---
# We isolate the 'Cv' column to see which features drive the Heat Capacity most.
target_corr = corr_matrix['Cv'].drop('Cv').sort_values(ascending=False)

print("\n--- Feature Importance: Correlation with Target ('Cv') ---")
print("   (Ordered from Most Positive to Most Negative)")
print("-" * 60)
print(target_corr.to_string())
print("-" * 60)

# ==========================================
# 11. VISUAL LINEARITY CHECK (Scatter Plots)
# ==========================================
# This cell generates scatter plots for every feature vs. the target.

# GOAL:
# 1. Check for Linearity: Linear models (Ridge, Lasso) assume a straight-line relationship.
# 2. Detect Outliers: Visual confirmation of data points far from the main cluster.

# --- 1. Setup Plotting Grid ---
import matplotlib.pyplot as plt
import seaborn as sns

feature_names = X_train_scaled.columns
N_FEATURES = len(feature_names)
N_COLS = 3
N_ROWS = 7  # 3 * 7 = 21 plots (Perfect fit)

print(f"--- Generating {N_FEATURES} Scatter Plots ---")
print("(Please wait ~10-20 seconds for rendering...)")

# Create the figure canvas
fig, axes = plt.subplots(N_ROWS, N_COLS, figsize=(18, 35))
axes_flat = axes.flatten()

# --- 2. Loop and Plot ---
for i, (feature_name, ax) in enumerate(zip(feature_names, axes_flat)):

    # VISUALIZATION TECHNIQUE (Big Data):
    # We use alpha=0.1 (transparency) because we have ~100,000 points.
    # Solid dots would look like a solid block of color.
    # Transparency allows us to see the DENSITY of the data.
    sns.scatterplot(
        x=X_train_scaled[feature_name],
        y=y_train,
        ax=ax,
        s=10,              # Dot size
        alpha=0.1,         # Transparency
        color='#2b7bba',   # Professional blue color
        edgecolor=None     # Remove outlines for faster rendering
    )

    # Formatting
    ax.set_title(f"Cv vs. {feature_name}", fontsize=12, fontweight='bold')
    ax.set_xlabel(f"Scaled {feature_name}", fontsize=10)
    ax.set_ylabel("Heat Capacity (Cv)", fontsize=10)
    ax.grid(True, linestyle='--', alpha=0.5)

# --- 3. Final Layout Adjustment ---
plt.tight_layout()
plt.subplots_adjust(top=0.97) # Make room for the main title
plt.suptitle("Feature Linearity Check: X vs Target (Cv)", fontsize=20, y=0.99)

plt.show()

# ==========================================
# 12. BASELINE MODEL (k-Nearest Neighbors)
# ==========================================
# This cell establishes a baseline performance benchmark using kNN.

# --- 1. Import Libraries ---
# from sklearn.neighbors import KNeighborsRegressor
# from sklearn.metrics import r2_score, mean_absolute_error

# --- 2. Initialize and Train ---
# Logic: "Similar molecules have similar properties." (Chemical Similarity Principle).
# We use k=5 and Distance-based weighting (default is uniform, but valid here).
# CRITICAL: kNN depends entirely on Euclidean distance, so 'X_train_scaled' is MANDATORY here.
print("Training baseline kNN model (k=5) on all 21 features...")
knn_baseline = KNeighborsRegressor(n_neighbors=5, n_jobs=-1)
knn_baseline.fit(X_train_scaled, y_train)

# --- 3. Evaluate on Test Set ---
# We predict on the unseen, scaled test data to measure generalization.
print("Evaluating baseline model on the test set...")
y_pred_baseline = knn_baseline.predict(X_test_scaled)

# --- 4. Report Performance (Metrics) ---
r2_baseline_knn = r2_score(y_test, y_pred_baseline)
mae_baseline_knn = mean_absolute_error(y_test, y_pred_baseline)

print("\n--- üèÅ Baseline kNN Performance (All 21 Features) ---")
print(f"Test Set R-squared (R2):            {r2_baseline_knn:.6f}")
print(f"Test Set Mean Absolute Error (MAE): {mae_baseline_knn:.6f} cal/mol¬∑K")

# ==========================================
# 13. FEATURE SELECTION (Lasso L1 Regularization)
# ==========================================
# This cell uses Lasso to automatically select the most important features.

# --- 1. Import the Lasso Model ---
from sklearn.linear_model import Lasso

# --- 2. Initialize and Train Lasso ---
# Lasso (Least Absolute Shrinkage and Selection Operator) is an "Embedded Method."
# It penalizes the absolute size of coefficients (L1 norm).
# KEY BENEFIT: It forces the coefficients of irrelevant features to become EXACTLY ZERO.
print("Training Lasso model for feature selection...")

# Parameters:
# - alpha=0.01: The strength of the penalty. Higher = fewer features selected.
# - max_iter=2000: Ensures the gradient descent algorithm has time to converge.
lasso = Lasso(alpha=0.01, random_state=42, max_iter=2000)
lasso.fit(X_train_scaled, y_train)

# --- 3. Analyze Coefficients ---
# We create a Series to map coefficients to their feature names.
coef_series = pd.Series(lasso.coef_, index=X_train.columns)

print("\n--- Lasso Feature Importance (Sorted) ---")
# Sorting helps us see the "Drivers" of Heat Capacity at the top/bottom.
print(coef_series.sort_values(ascending=False))

# --- 4. Filter Selected Features ---
# We keep only features where the coefficient is NOT zero.
# (We use a small threshold 1e-5 to account for floating-point precision errors).
selected_features_lasso = coef_series[abs(coef_series) > 1e-5]
removed_features_lasso = coef_series[abs(coef_series) <= 1e-5]

print("\n--- üîé Lasso Feature Selection Results ---")
print(f"Total features selected: {len(selected_features_lasso)} / {len(X_train.columns)}")
print(f"Total features removed:  {len(removed_features_lasso)}")

print("\n--- SELECTED FEATURES (To be used in Final Model) ---")
print(selected_features_lasso.index.tolist())

print("\n--- REMOVED FEATURES (Deemed irrelevant by Lasso) ---")
print(removed_features_lasso.index.tolist())

# ==========================================
# 14. MODEL COMPARISON (Full vs. Selected Features)
# ==========================================
# This cell tests if a simpler model (fewer features) performs as well as the complex one.

# --- 1. Define Selected Features ---
# BEST PRACTICE: Use the variable from the previous cell to ensure consistency.

try:
    selected_features_list = selected_features_lasso.index.tolist()
    print(f"Using {len(selected_features_list)} features dynamically selected by Lasso.")
except NameError:
    # Fallback if Cell 13 variables are lost
    selected_features_list = [
        'MolecularWeight', 'AtomCount', 'BondCount', 'mu', 'alpha',
        'homo', 'lumo', 'r2', 'zpve', 'U_atom', 'G_atom',
        'A', 'B', 'C'
    ]
    print(f"Using hardcoded list of {len(selected_features_list)} features.")

# --- 2. Filter Data ---
# We create new subsets containing ONLY the selected features.
X_train_sel = X_train_scaled[selected_features_list]
X_test_sel = X_test_scaled[selected_features_list]

print(f"New Feature Matrix Shape: {X_train_sel.shape}")

# --- 3. Train New kNN Model ---
# We use the EXACT same hyperparameters (k=5) to ensure a fair comparison.
print("\n--- Training kNN on Reduced Feature Set ---")
knn_selected = KNeighborsRegressor(n_neighbors=5, n_jobs=-1)
knn_selected.fit(X_train_sel, y_train)

# --- 4. Evaluate ---
print("Evaluating performance...")
y_pred_selected = knn_selected.predict(X_test_sel)

r2_knn_selected = r2_score(y_test, y_pred_selected)
mae_knn_selected = mean_absolute_error(y_test, y_pred_selected)

# --- 5. Performance Comparison Table ---
# This output is perfect for your Final Report "Results" table.
print("\n\n--- üèÅ MODEL COMPARISON: COMPLEXITY VS ACCURACY üèÅ ---")
print(f"{'Metric':<25} | {'Full Model (21 Feat.)':<20} | {'Selected (Lasso Subset)':<20}")
print("-" * 75)
print(f"{'R-squared (R2)':<25} | {r2_baseline_knn:.6f}             | {r2_knn_selected:.6f}")
print(f"{'MAE (cal/mol¬∑K)':<25} | {mae_baseline_knn:.6f}             | {mae_knn_selected:.6f}")

# --- 6. Automated Analysis (for Discussion Section) ---
# We calculate the "Cost" of simplification.
r2_drop = r2_baseline_knn - r2_knn_selected
mae_increase = mae_knn_selected - mae_baseline_knn

print("\n--- Analysis of Trade-off ---")
print(f"Feature Reduction: Dropped {len(X_train.columns) - len(selected_features_list)} features.")
print(f"Performance Cost:  R2 dropped by {r2_drop:.5f} | MAE increased by {mae_increase:.5f}")

if r2_drop < 0.01:
    print("\n‚úÖ CONCLUSION: Success! The model became much simpler (Parsimonious)")
    print("   with negligible loss in accuracy. We should use the Selected Features moving forward.")
else:
    print("\n‚ö†Ô∏è CONCLUSION: The drop in accuracy is noticeable. You must decide in your report")
    print("   if the simplicity is worth the loss in precision.")

# ==========================================
# 15. MODEL COMPARISON: LINEAR (Ridge) vs NON-LINEAR (kNN)
# ==========================================
# This cell tests a linear model (Ridge/L2) to determine the nature of the problem.

# --- 1. Import Libraries ---
# from sklearn.linear_model import RidgeCV
# from sklearn.metrics import r2_score, mean_absolute_error
# import numpy as np

# --- 2. Initialize and Train Ridge (L2) ---
# Ridge Regression adds an L2 penalty to shrink coefficients.
# It handles Multicollinearity (correlated features) better than standard Linear Regression.
print("Training Ridge (L2) Regression model with Cross-Validation...")

# Hyperparameter Tuning:
# We test 'alpha' values from 0.01 to 1000 to find the best balance between
# bias (underfitting) and variance (overfitting).
alphas_to_test = np.logspace(-2, 3, 10)

# RidgeCV performs efficient Cross-Validation to select the best alpha.
ridge_cv_model = RidgeCV(
    alphas=alphas_to_test,
    scoring='r2',
    cv=5  # 5-Fold CV ensures robustness
)

# Fit on the FULL 21-feature scaled training data
ridge_cv_model.fit(X_train_scaled, y_train)

# --- 3. Evaluate Performance ---
print(f"\n--- Best Hyperparameters Found ---")
print(f"Optimal Regularization Strength (alpha): {ridge_cv_model.alpha_:.4f}")

print("\nEvaluating Ridge model on Test Set...")
y_pred_ridge = ridge_cv_model.predict(X_test_scaled)

r2_ridge = r2_score(y_test, y_pred_ridge)
mae_ridge = mean_absolute_error(y_test, y_pred_ridge)

# --- 4. Comparison Table (The "Science" Step) ---
# We compare the Linear assumption (Ridge) vs the Non-Linear assumption (kNN).
print("\n\n--- üèÅ LINEAR vs. NON-LINEAR COMPARISON üèÅ ---")
print(f"{'Metric':<25} | {'kNN (Non-Linear)':<20} | {'Ridge (Linear)':<20}")
print("-" * 75)
print(f"{'R-squared (R2)':<25} | {r2_knn_selected:.6f}             | {r2_ridge:.6f}")
print(f"{'MAE (cal/mol¬∑K)':<25} | {mae_knn_selected:.6f}             | {mae_ridge:.6f}")

# --- 5. Automated Scientific Interpretation ---
# This logic specifically handles your negative R2 result (-308).
print("\n--- Scientific Interpretation ---")

if r2_ridge < 0:
    print("‚ùå CRITICAL FINDING: The Linear Model (Ridge) Failed Catastrophically.")
    print(f"   R2 Score: {r2_ridge:.4f} (Negative R2 means worse than a horizontal line).")
    print("\n   SCIENTIFIC CONCLUSION:")
    print("   1. The relationship between molecular features and Heat Capacity is")
    print("      STRONGLY NON-LINEAR.")
    print("   2. A linear combination of features cannot capture the underlying physics.")
    print("   3. This result provides strong justification for moving to Ensemble Methods")
    print("      (Random Forest / XGBoost) which can model complex, non-linear boundaries.")
elif r2_ridge < r2_knn_selected:
    print("‚ÑπÔ∏è FINDING: The Non-Linear model (kNN) outperforms the Linear model.")
    print("   This confirms that non-linear interactions are dominant.")
else:
    print("‚ÑπÔ∏è FINDING: The Linear model performs well.")

# ==========================================
# 16. ADVANCED MODELING (XGBoost Ensemble)
# ==========================================
# This cell implements Gradient Boosting to capture non-linear relationships.

# --- 1. Setup ---
# XGBoost (Extreme Gradient Boosting) is an industry-standard ensemble method.
# It builds trees sequentially, where each new tree corrects the errors of the previous ones.
try:
    import xgboost as xgb
except ImportError:
    import xgboost as xgb

from sklearn.metrics import r2_score, mean_absolute_error

# --- 2. Initialize and Train XGBoost ---
print("Training XGBoost Regressor on ALL 21 features...")

# Parameters:
# - n_jobs=-1: Use all CPU cores for speed.
# - random_state=42: Ensure reproducibility.
# Note: We use default hyperparameters first. We will tune them in the next step.
xgb_model_21 = xgb.XGBRegressor(
    n_estimators=100,  # Number of trees
    learning_rate=0.1, # Step size shrinkage
    max_depth=6,       # Tree depth (control complexity)
    random_state=42,
    n_jobs=-1
)

# Fit on the FULL scaled training data
# (Ensembles handle unscaled data well, but using scaled is fine too)
xgb_model_21.fit(X_train_scaled, y_train)

# --- 3. Evaluate Performance ---
print("Evaluating XGBoost on Test Set...")
y_pred_xgb_21 = xgb_model_21.predict(X_test_scaled)

r2_xgb_21 = r2_score(y_test, y_pred_xgb_21)
mae_xgb_21 = mean_absolute_error(y_test, y_pred_xgb_21)

print("\n--- üöÄ XGBoost Performance (All 21 Features) ---")
print(f"Test Set R-squared (R2):            {r2_xgb_21:.6f}")
print(f"Test Set Mean Absolute Error (MAE): {mae_xgb_21:.6f} cal/mol¬∑K")

# --- 4. Performance Comparison (The "Result" Table) ---
print("\n\n--- üèÅ FINAL MODEL LEADERBOARD üèÅ ---")
print(f"{'Model':<25} | {'R2 Score':<15} | {'MAE (cal/mol¬∑K)':<15}")
print("-" * 65)
# We compare against our best previous model (kNN)
print(f"{'kNN (14 Selected Feat.)':<25} | {r2_knn_selected:.6f}          | {mae_knn_selected:.6f}")
print(f"{'XGBoost (21 All Feat.)':<25} | {r2_xgb_21:.6f}          | {mae_xgb_21:.6f}")

# --- 5. Scientific Interpretation ---
print("\n--- Interpretation ---")
improvement = mae_knn_selected - mae_xgb_21

if improvement > 0:
    print(f"‚úÖ SUCCESS: XGBoost reduced the error by {improvement:.4f} cal/mol¬∑K compared to kNN.")
    print("   Reasoning: Gradient Boosting effectively captures the complex, non-linear")
    print("   interactions between molecular descriptors that the simpler kNN missed.")
else:
    print(f"‚ÑπÔ∏è RESULT: XGBoost performed similarly to kNN (Diff: {improvement:.4f}).")
    print("   Reasoning: The dataset might be dense enough that 'distance' (kNN)")
    print("   is just as good as 'rules' (XGBoost) for this specific property.")

# ==========================================
# 17. FEATURE IMPORTANCE ANALYSIS (XGBoost)
# ==========================================
# This cell analyzes which features the XGBoost model relies on most.

# --- 1. Extract Feature Importances ---
print("--- üîé XGBoost Feature Importance Analysis ---")
# 'Gain' implies the average improvement in accuracy (or reduction in error)
# brought by a feature to the branches it is on.
importances = xgb_model_21.feature_importances_

# Map importances to feature names
importance_series = pd.Series(importances, index=X_train_scaled.columns)

print("Top 10 Features by 'Gain' (Predictive Power):")
# Sorting helps us identify the primary physical drivers of Heat Capacity.
print(importance_series.sort_values(ascending=False).head(10))

# --- 2. Visualization ---
print("\n--- Plotting Feature Importance ---")
fig, ax = plt.subplots(figsize=(12, 10))

# We use 'gain' because it shows the *quality* of the split, not just the quantity.
xgb.plot_importance(
    xgb_model_21,
    ax=ax,
    importance_type='gain',
    title="XGBoost Feature Importance (Gain)",
    show_values=False,       # Hides numbers to keep plot clean
    color='#2b7bba',         # Professional blue
    grid=False               # Cleaner look
)
plt.title("XGBoost Feature Importance (Gain) - Drivers of Heat Capacity", fontsize=15)
plt.xlabel("F Score (Gain)", fontsize=12)
plt.ylabel("Features", fontsize=12)
plt.show()

# --- 3. Scientific Interpretation (for Report) ---
top_feature = importance_series.idxmax()
top_score = importance_series.max()

print("\n--- Interpretation ---")
print(f"1. The Dominant Feature is '{top_feature}' with a score of {top_score:.4f}.")
print("   This suggests that this physical property is the primary determinant")
print("   of Heat Capacity in the QM9 dataset.")

# ==========================================
# 18. MODEL OPTIMIZATION (Top 10 Features)
# ==========================================
# This cell tests if we can maintain accuracy while removing 50% of the inputs.

# --- 1. Import Libraries ---
from sklearn.metrics import r2_score, mean_absolute_error
import xgboost as xgb

# --- 2. Automated Feature Selection ---
print("--- 1. Selecting Top 10 Features (based on Gain) ---")
try:
    # We dynamically pull the top 10 features from the previous cell's analysis.
    # This ensures your pipeline is "connected" and reproducible.
    top_10_features = importance_series.sort_values(ascending=False).head(10).index.tolist()

    print(f"Selected Features (Top 10):")
    print(top_10_features)

except NameError:
    print("‚ùå ERROR: 'importance_series' not found. Please run Cell 17 first.")
    # Fallback for safety if previous cell wasn't run
    top_10_features = [
        'MolecularWeight', 'AtomCount', 'BondCount', 'mu', 'alpha',
        'homo', 'lumo', 'gap', 'zpve', 'U0'
    ]
    print("‚ö†Ô∏è Using fallback feature list.")

# --- 3. Create Optimized Datasets ---
# Reducing dimensionality speeds up training and simplifies data collection.
X_train_opt = X_train_scaled[top_10_features]
X_test_opt = X_test_scaled[top_10_features]

print(f"\nOptimized Training Shape: {X_train_opt.shape}")

# --- 4. Train & Evaluate Optimized XGBoost ---
print("Training XGBoost on Reduced Feature Set...")
xgb_model_top10 = xgb.XGBRegressor(random_state=42, n_jobs=-1)
xgb_model_top10.fit(X_train_opt, y_train)

y_pred_xgb_top10 = xgb_model_top10.predict(X_test_opt)

# Metrics
r2_xgb_top10 = r2_score(y_test, y_pred_xgb_top10)
mae_xgb_top10 = mean_absolute_error(y_test, y_pred_xgb_top10)

print("\n--- üöÄ XGBoost Performance (Top 10 Features) ---")
print(f"Test Set R-squared (R2):            {r2_xgb_top10:.6f}")
print(f"Test Set Mean Absolute Error (MAE): {mae_xgb_top10:.6f} cal/mol¬∑K")

# --- 5. Final Leaderboard Comparison ---
print("\n\n--- üèÅ FINAL MODEL LEADERBOARD üèÅ ---")
print(f"{'Model':<25} | {'R2 Score':<12} | {'MAE':<12} | {'Features':<10}")
print("-" * 70)
# Specific labels as requested
print(f"{'kNN (14 feat.)':<25} | {r2_knn_selected:.4f}       | {mae_knn_selected:.4f}       | 14")
print(f"{'XGBoost (21 feat.)':<25} | {r2_xgb_21:.4f}       | {mae_xgb_21:.4f}       | 21")
print(f"{'XGBoost (10 feat.)':<25} | {r2_xgb_top10:.4f}       | {mae_xgb_top10:.4f}       | 10")

# --- 6. Engineering Trade-off Analysis ---
print("\n--- Engineering Analysis ---")
r2_diff = r2_xgb_top10 - r2_xgb_21
mae_diff = mae_xgb_top10 - mae_xgb_21

print(f"Impact of removing 11 features (52% reduction in data requirements):")
print(f" - R2 Change:  {r2_diff:+.6f}")
print(f" - MAE Change: {mae_diff:+.6f} cal/mol¬∑K")

if abs(r2_diff) < 0.01:
    print("\n‚úÖ CONCLUSION (Parsimony): The 'XGBoost (10 feat.)' model is superior.")
    print("   We cut the data requirements by half with negligible loss in accuracy.")
    print("   This reduces computational cost and experimental effort.")
else:
    print("\n‚ö†Ô∏è CONCLUSION (Trade-off): Accuracy dropped slightly.")
    print("   Is saving 11 data inputs worth the increase in error? Discuss this in your report.")

# ==========================================
# 19. HYPERPARAMETER TUNING (Grid Search)
# ==========================================
# This cell optimizes the XGBoost model to find the best configuration.

# --- 1. Import Libraries ---
# from sklearn.model_selection import GridSearchCV
# from sklearn.metrics import r2_score, mean_absolute_error

warnings.filterwarnings('ignore') # Clean output

# --- 2. Define the Hyperparameter Grid ---
# We test different combinations of tree depth, count, and learning speed.
param_grid = {
    'max_depth': [3, 5, 7],         # Controls complexity (Low = Underfit, High = Overfit)
    'n_estimators': [100, 200],     # Number of trees (More is usually better but slower)
    'learning_rate': [0.01, 0.1]    # Step size (Lower needs more trees)
}

# --- 3. Initialize Grid Search ---
# We tune the "Optimized" model (Top 10 Features) because it is our most efficient candidate.
base_xgb_top10 = xgb.XGBRegressor(random_state=42, n_jobs=-1)

grid_search_top10 = GridSearchCV(
    estimator=base_xgb_top10,
    param_grid=param_grid,
    cv=5,                 # 5-Fold Cross-Validation
    scoring='r2',         # Optimize for R-squared
    n_jobs=-1,
    verbose=1             # Show progress
)

# --- 4. Execute Tuning ---
print("--- Starting Hyperparameter Tuning on 10 Features... ---")
print("(This process trains multiple models to find the best one)")
grid_search_top10.fit(X_train_opt, y_train) # fitting on 'X_train_opt' (created in Cell 18)

# --- 5. Report Best Parameters ---
print("\n--- ‚úÖ Tuning Complete! ---")
print("Best Hyperparameters Found:")
print(grid_search_top10.best_params_)
print(f"Best Cross-Validation Score (R2): {grid_search_top10.best_score_:.6f}")

# --- 6. Evaluate the Best Model ---
# GridSearchCV automatically refits the best model on the full training set.
xgb_tuned_top10 = grid_search_top10.best_estimator_

print("\nEvaluating Tuned Model on Test Set...")
y_pred_xgb_tuned = xgb_tuned_top10.predict(X_test_opt)

# NAMING UPDATE: Explicitly labeling these as XGB metrics
r2_xgb_tuned_top10 = r2_score(y_test, y_pred_xgb_tuned)
mae_xgb_tuned_top10 = mean_absolute_error(y_test, y_pred_xgb_tuned)

# --- 7. FINAL MODEL SHOWDOWN ---
# This table summarizes your entire project journey.
print("\n\n--- üèÜ FINAL MODEL SHOWDOWN üèÜ ---")
# Define column widths for alignment
h_fmt = "{:<20} | {:<20} | {:<20} | {:<20}"
row_fmt = "{:<20} | {:<20.6f} | {:<20.6f} | {:<20.6f}"

print(h_fmt.format("Metric", "kNN (14 feat.)", "XGB (10 feat.)", "XGB-Tuned (10 feat.)"))
print("-" * 90)
# Using the specific variable names to ensure accuracy
print(row_fmt.format("R-squared (R2)", r2_knn_selected, r2_xgb_top10, r2_xgb_tuned_top10))
print(row_fmt.format("MAE (cal/mol¬∑K)", mae_knn_selected, mae_xgb_top10, mae_xgb_tuned_top10))

# --- 8. Scientific Conclusion ---
print("\n--- Final Analysis ---")
if r2_xgb_tuned_top10 > r2_xgb_top10:
    print("‚úÖ CONCLUSION: Hyperparameter tuning successfully improved the model.")
    print(f"   We achieved a lower MAE of {mae_xgb_tuned_top10:.4f} with only 10 features.")
    print("   This represents the optimal balance of Accuracy vs. Complexity.")
else:
    print("‚ÑπÔ∏è CONCLUSION: Default parameters were already optimal.")
    print("   The tuned model performed similarly, confirming the robustness of XGBoost.")

# ==========================================
# 20. MODEL COMPARISON: BAGGING (Random Forest)
# ==========================================
# This cell tests if "Bagging" (Random Forest) outperforms "Boosting" (XGBoost).

# --- 1. Import Libraries ---
# from sklearn.ensemble import RandomForestRegressor
# from sklearn.metrics import r2_score, mean_absolute_error

# --- 2. Initialize and Train Random Forest ---
# Random Forest (Bagging) builds independent trees in parallel and averages them.
# It is generally more robust to overfitting than un-tuned XGBoost but often slower.
print("Training Random Forest Regressor on ALL 21 features...")
print("(This uses parallel processing [n_jobs=-1] to speed up tree building)")

rf_model_21 = RandomForestRegressor(
    n_estimators=100, # Standard default
    random_state=42,  # Reproducibility
    n_jobs=-1
)

# Fit on the full 21-feature training set for a fair "Base vs Base" comparison
rf_model_21.fit(X_train_scaled, y_train)

# --- 3. Evaluate Performance ---
print("Evaluating Random Forest on Test Set...")
y_pred_rf_21 = rf_model_21.predict(X_test_scaled)

r2_rf_21 = r2_score(y_test, y_pred_rf_21)
mae_rf_21 = mean_absolute_error(y_test, y_pred_rf_21)

print("\n--- üå≥ Random Forest Performance (All 21 Features) ---")
print(f"Test Set R-squared (R2):            {r2_rf_21:.6f}")
print(f"Test Set Mean Absolute Error (MAE): {mae_rf_21:.6f} cal/mol¬∑K")

# --- 4. Final Model Showdown ---
# We compare the Best Previous Model (Tuned XGBoost) vs. the New Challenger (RF).
print("\n\n--- üèÅ FINAL MODEL SHOWDOWN üèÅ ---")
# Formatting for clear alignment
h_fmt = "{:<20} | {:<20} | {:<22} | {:<20}"
row_fmt = "{:<20} | {:<20.6f} | {:<22.6f} | {:<20.6f}"

print(h_fmt.format("Metric", "kNN (14 feat.)", "XGB-Tuned (10 feat.)", "RF-Base (21 feat.)"))
print("-" * 90)
# Note: We use 'r2_xgb_tuned_top10' which defines the specific optimized model
print(row_fmt.format("R-squared (R2)", r2_knn_selected, r2_xgb_tuned_top10, r2_rf_21))
print(row_fmt.format("MAE (cal/mol¬∑K)", mae_knn_selected, mae_xgb_tuned_top10, mae_rf_21))

# --- 5. Scientific Interpretation (Bagging vs Boosting) ---
print("\n--- Scientific Conclusion ---")
r2_diff = r2_rf_21 - r2_xgb_tuned_top10

if r2_diff > 0.001:
    print("‚úÖ NEW CHAMPION: Random Forest (Bagging) is superior.")
    print(f"   It outperformed the tuned XGBoost by {r2_diff:+.4f} R2.")
    print("   Interpretation: The variance reduction from averaging full-depth trees")
    print("   was more effective than the bias reduction from boosting shallow trees.")
else:
    print("‚ÑπÔ∏è RESULT: The Tuned XGBoost (Boosting) remains the champion.")
    print("   It outperformed (or matched) Random Forest with FEWER features (10 vs 21).")
    print("   Interpretation: The sequential error-correction of Boosting was more efficient.")
    print("   Engineering Value: We achieved better/similar results with 50% less data.")

# ==========================================
# 21. FEATURE IMPORTANCE ANALYSIS (Random Forest)
# ==========================================
# This cell analyzes which features "Bagging" (Random Forest) relies on most.

# --- 1. Extract Feature Importances ---
# Random Forest uses "Mean Decrease in Impurity" (MDI).
# It measures how much each feature reduces the variance (uncertainty) in the trees.
print("--- üîé Random Forest Feature Importance Analysis ---")

# Get importances from the trained model (rf_model_21)
importances_rf = rf_model_21.feature_importances_
importance_series_rf = pd.Series(importances_rf, index=X_train_scaled.columns)

# Sort from most to least important
importance_series_rf_sorted = importance_series_rf.sort_values(ascending=False)

print("Random Forest Feature Importances (Top 10):")
print(importance_series_rf_sorted.head(10))

# --- 2. Visualization ---
# Visualizing importances helps us confirm if the model makes physical sense.
print("\n--- Plotting Random Forest Feature Importances ---")
plt.figure(figsize=(12, 8))

# Use Seaborn for a clean, professional bar plot
sns.barplot(
    x=importance_series_rf_sorted.values,
    y=importance_series_rf_sorted.index,
  )

plt.title("Random Forest Feature Importance (Mean Decrease in Impurity)", fontsize=15)
plt.xlabel("Importance (MDI)", fontsize=12)
plt.ylabel("Features", fontsize=12)
plt.show()

# ==========================================
# 22. MODEL OPTIMIZATION (Random Forest Top 10)
# ==========================================
# This cell tests if Random Forest can maintain performance with 50% fewer inputs.

# --- 1. Import Libraries ---
# from sklearn.ensemble import RandomForestRegressor
# from sklearn.metrics import r2_score, mean_absolute_error

# --- 2. Filter Data (Feature Selection) ---
print(f"--- 1. Reducing Data to Top 10 RF Features ---")

# FIX: Define the variable explicitly if missing
if 'top_10_rf_features' not in locals():
    # Use the sorted series from the previous cell if available
    if 'importance_series_rf_sorted' in locals():
        top_10_rf_features = importance_series_rf_sorted.head(10).index.tolist()
        print("Generated 'top_10_rf_features' from 'importance_series_rf_sorted'.")
    else:
        # Fallback hardcoded list based on typical RF outputs for QM9
        top_10_rf_features = ['AtomCount', 'r2', 'A', 'zpve', 'MolecularWeight',
                              'BondCount', 'C', 'lumo', 'B', 'alpha']
        print("‚ö†Ô∏è Warning: 'importance_series_rf_sorted' not found. Using fallback list.")

print(f"Selected Features: {top_10_rf_features}")

X_train_rf_top10 = X_train_scaled[top_10_rf_features]
X_test_rf_top10 = X_test_scaled[top_10_rf_features]

print(f"New training data shape: {X_train_rf_top10.shape}")

# --- 3. Train Optimized Random Forest ---
print("Training Random Forest model on Top 10 features...")
rf_model_top10 = RandomForestRegressor(random_state=42, n_jobs=-1)
rf_model_top10.fit(X_train_rf_top10, y_train)

# --- 4. Evaluate ---
print("Evaluating Top 10 RF model on the test set...")
y_pred_rf_top10 = rf_model_top10.predict(X_test_rf_top10)

r2_rf_top10 = r2_score(y_test, y_pred_rf_top10)
mae_rf_top10 = mean_absolute_error(y_test, y_pred_rf_top10)

print("\n--- üå≥ Random Forest Performance (Top 10 Features) ---")
print(f"Test Set R-squared (R2):            {r2_rf_top10:.6f}")
print(f"Test Set Mean Absolute Error (MAE): {mae_rf_top10:.6f} cal/mol¬∑K")

# --- 5. FINAL CHAMPIONSHIP SHOWDOWN ---
print("\n\n--- üèÜ GRAND FINALE: MODEL LEADERBOARD üèÜ ---")
# Formatting
h_fmt = "{:<20} | {:<20} | {:<22} | {:<20}"
row_fmt = "{:<20} | {:<20.6f} | {:<22.6f} | {:<20.6f}"

print(h_fmt.format("Metric", "kNN (14 feat.)", "XGB-Tuned (10 feat.)", "RF-Base (10 feat.)"))
print("-" * 90)
print(row_fmt.format("R-squared (R2)", r2_knn_selected, r2_xgb_tuned_top10, r2_rf_top10))
print(row_fmt.format("MAE (cal/mol¬∑K)", mae_knn_selected, mae_xgb_tuned_top10, mae_rf_top10))

# --- 6. Final Decision Analysis ---
print("\n--- Final Engineering Decision ---")
r2_diff = r2_rf_top10 - r2_xgb_tuned_top10

if r2_diff > 0.001:
    print("‚úÖ CONCLUSION: The Random Forest (10 feat.) is the GRAND CHAMPION!")
    print(f"   It beat the tuned XGBoost by {r2_diff:.4f} R2 points.")
    print("   Recommendation: Deploy this Random Forest model for production.")
elif abs(r2_diff) < 0.001:
    print("‚ÑπÔ∏è CONCLUSION: Tie Game.")
    print("   XGBoost and Random Forest perform almost identically on the reduced set.")
    print("   Recommendation: XGBoost is often preferred for deployment (faster inference speed).")
else:
    print("‚úÖ CONCLUSION: The Tuned XGBoost (10 feat.) remains the GRAND CHAMPION!")
    print("   Random Forest lost some accuracy when features were removed.")
    print("   Recommendation: Deploy the Tuned XGBoost model.")

# ==========================================
# 23. HYPERPARAMETER TUNING (Random Forest)
# ==========================================
# This cell optimizes the Random Forest model to see if it can beat XGBoost.

# --- 1. Import Libraries ---
# from sklearn.model_selection import GridSearchCV
# from sklearn.ensemble import RandomForestRegressor
# from sklearn.metrics import r2_score, mean_absolute_error
import joblib  # For saving the final champion model
# import warnings

warnings.filterwarnings('ignore')

# --- 2. Define "Fast" Hyperparameter Grid ---
# ENGINEERING DECISION:
# Random Forest tuning can be very slow. We use a reduced grid (50 trees)
# to check if optimization yields significant gains without waiting hours.
# (Trade-off: Computation Time vs. Hyperparameter Exhaustiveness)
param_grid_rf_fast = {
    'n_estimators': [50],        # Reduced from 100 to speed up search 2x
    'max_depth': [20, None],     # Limit depth to prevent overfitting vs. Full depth
    'max_features': ['sqrt']     # Standard heuristic for regression
}

# --- 3. Initialize Grid Search ---
base_rf_top10 = RandomForestRegressor(random_state=42, n_jobs=-1)

grid_search_rf = GridSearchCV(
    estimator=base_rf_top10,
    param_grid=param_grid_rf_fast,
    cv=3,                 # 3-Fold CV is faster than 5-Fold (valid for quick checks)
    scoring='r2',
    n_jobs=-1,
    verbose=1             # Reduced verbosity
)

# --- 4. Execute Tuning ---
print("--- Starting Fast Hyperparameter Tuning (Random Forest)... ---")
grid_search_rf.fit(X_train_rf_top10, y_train)

# --- 5. Report Best Parameters ---
print("\n--- ‚úÖ Tuning Complete! ---")
print("Best Random Forest Parameters:")
print(grid_search_rf.best_params_)
print(f"Best Cross-Validation Score (R2): {grid_search_rf.best_score_:.6f}")

# --- 6. Evaluate the Best Model ---
best_tuned_rf = grid_search_rf.best_estimator_

print("\nEvaluating Tuned RF Model on Test Set...")
y_pred_rf_tuned = best_tuned_rf.predict(X_test_rf_top10)

r2_rf_tuned_top10 = r2_score(y_test, y_pred_rf_tuned)
mae_rf_tuned_top10 = mean_absolute_error(y_test, y_pred_rf_tuned)

# --- 7. FINAL GRAND SHOWDOWN ---
print("\n\n--- üèÜ ULTIMATE MODEL LEADERBOARD üèÜ ---")
h_fmt = "{:<20} | {:<20} | {:<22} | {:<20}"
row_fmt = "{:<20} | {:<20.6f} | {:<22.6f} | {:<20.6f}"

print(h_fmt.format("Metric", "kNN (14 feat.)", "XGB-Tuned (10 feat.)", "RF-Tuned (10 feat.)"))
print("-" * 90)
print(row_fmt.format("R-squared (R2)", r2_knn_selected, r2_xgb_tuned_top10, r2_rf_tuned_top10))
print(row_fmt.format("MAE (cal/mol¬∑K)", mae_knn_selected, mae_xgb_tuned_top10, mae_rf_tuned_top10))

# --- 8. Final Decision & Model Saving ---
print("\n--- Final Decision ---")
# We determine the absolute best model to save

champion_model = xgb_tuned_top10
model_name = "XGBoost"
print("‚úÖ CONCLUSION: The Tuned XGBoost remains the BEST model.")

# Rubric Requirement: Code Package (Save the best model)
filename = f'champion_model_{model_name}.joblib'
joblib.dump(champion_model, filename)
print(f"\nüíæ SAVED: The champion model has been saved to '{filename}'.")

# ==========================================
# 24. MODEL INTERPRETABILITY (SHAP Analysis)
# ==========================================
# This cell uses Game Theory (Shapley Values) to explain HOW the model thinks.

# --- 1. Setup & Installation ---
try:
    import shap
except ImportError:
    print("Installing SHAP library...")
    import shap

import matplotlib.pyplot as plt

# --- 2. Load Champion Model & Data ---
# We interpret the 'xgb_tuned_top10' (tuned XGBoost model).
# Ensure we use the optimized 10-feature dataset.
model_to_explain = xgb_tuned_top10  # Use the tuned XGBoost
data_for_shap = X_train_opt            # The 10-feature training data

print(f"Explaining Model: {type(model_to_explain).__name__}")
print(f"Data Shape: {data_for_shap.shape}")

# --- 3. Compute SHAP Values ---
# TreeExplainer is optimized for XGBoost/RandomForest.
# It calculates the marginal contribution of each feature to the prediction.
explainer = shap.TreeExplainer(model_to_explain)
shap_values = explainer(data_for_shap)

print("‚úÖ SHAP values calculated successfully.")

# --- 4. GLOBAL Interpretation (Beeswarm Plot) ---
# This plot summarizes the entire dataset.
# - Y-axis: Features ranked by importance.
# - Color: Feature Value (Red = High, Blue = Low).
# - X-axis: Impact on Heat Capacity (Positive = Increases Cv, Negative = Decreases Cv).
print("\n--- 1. Global Feature Importance (Beeswarm) ---")
plt.figure(figsize=(12, 8))
plt.title("Global Feature Impact on Heat Capacity (Cv)", fontsize=20)

shap.plots.beeswarm(
    shap_values,
    max_display=10,
    show=False,
    plot_size=(12, 6)
)
plt.xticks(fontsize=16)
plt.subplots_adjust(left=0.30, right=0.95, top=0.9, bottom=0.1)

plt.show()

# --- 5. LOCAL Interpretation (Waterfall Plot) ---
# This explains a SINGLE prediction (e.g., the first molecule).
# It shows how we get from the average Cv to the specific Cv for this molecule.
print("\n--- 2. Local Prediction Explanation (Molecule #0) ---")
print(f"Actual Prediction: {model_to_explain.predict(data_for_shap.iloc[[0]])[0]:.4f}")

plt.figure(figsize=(12, 6))
plt.title(f"Why did the model predict this value for Molecule 0?", fontsize=18)

shap.plots.waterfall(
    shap_values[0],
    max_display=10,
    show=False
)
plt.xticks(fontsize=16)
plt.subplots_adjust(left=0.3, right=0.95, top=0.9, bottom=0.1)

plt.show()

# --- 6. Scientific Interpretation Guide ---
print("\n--- üß™ HOW TO INTERPRET (Write this in your report) ---")
print("Look at the Beeswarm Plot (Top Plot):")
print("1. FIND THE TOP FEATURE (e.g., AtomCount or MolecularWeight).")
print("2. CHECK THE COLOR:")
print("   - If High values (RED) are on the RIGHT side (Positive SHAP):")
print("     It means 'Increasing this feature INCREASES Heat Capacity.'")
print("   - If High values (RED) are on the LEFT side (Negative SHAP):")
print("     It means 'Increasing this feature DECREASES Heat Capacity.'")
print("\nExample Logic: 'As expected, larger molecules (High AtomCount/Red) have")
print("higher Heat Capacity (Positive SHAP), aligning with thermodynamic theory.'")

# ==========================================
# 25. FINAL VALIDATION & UNCERTAINTY ESTIMATION
# ==========================================
# Methodology: Bootstrap Ensemble with Feature Subsampling.

# --- 1. Import Libraries ---
# from sklearn.utils import resample
# from sklearn.base import clone
from scipy.stats import pearsonr

# --- 2. Configuration & Setup ---
N_BOOTSTRAPS = 25          # Number of models in the ensemble (Higher = More stable)
FEATURE_FRACTION = 0.80    # Train each model on 80% of features to force diversity

# Robustly load the Champion Model
try:
    BASE_MODEL = xgb_tuned_top10
    print(f"Loaded Model: Tuned XGBoost ({type(BASE_MODEL).__name__})")
except NameError:
    try:
        BASE_MODEL = xgb_tuned_top10
        print("Loaded Model: Best Tuned XGBoost (Fallback)")
    except NameError:
        print("‚ùå ERROR: No tuned model found. Please run Cell 19.")
        raise

# Robustly load Optimized Data
try:
    X_tr = X_train_opt
    X_te = X_test_opt
    print(f"Using Optimized Data: {X_tr.shape}")
except NameError:
    print("‚ùå ERROR: Optimized data not found. Please run Cell 18.")
    raise

print(f"\n--- üöÄ Starting Ensemble Validation (Models={N_BOOTSTRAPS}) ---")

# --- 3. Ensemble Training (Bootstrapping) ---
ensemble_preds = []

# We train N independent models on slightly different versions of the data
for i in range(N_BOOTSTRAPS):
    # A. Data Diversity: Resample training data (Bootstrapping)
    X_boot, y_boot = resample(X_tr, y_train, random_state=i)

    # B. Model Diversity: Inject randomness into the model itself
    model_boot = clone(BASE_MODEL)

    # We update parameters to force decorrelation (essential for ensembles)
    # If using XGBoost, we use 'colsample_bytree'. If Random Forest, 'max_features'.
    try:
        model_params = BASE_MODEL.get_params()
        if 'xgb' in str(type(BASE_MODEL)).lower():
            model_params.update({'colsample_bytree': FEATURE_FRACTION, 'random_state': i})
        else:
            model_params.update({'max_features': FEATURE_FRACTION, 'random_state': i})

        model_boot.set_params(**model_params)
    except Exception:
        pass # Fallback if parameter update fails

    # C. Train & Predict
    model_boot.fit(X_boot, y_boot)
    ensemble_preds.append(model_boot.predict(X_te))

    if (i + 1) % 5 == 0:
        print(f"   > Progress: Trained Model {i+1}/{N_BOOTSTRAPS}...")

# Convert list to matrix: Shape (25, N_Test_Samples)
ensemble_matrix = np.array(ensemble_preds)

# --- 4. Aggregate Results ---
# The Mean is our Final Prediction.
# The Standard Deviation is our Measure of Uncertainty (Epistemic).
y_pred_mean = np.mean(ensemble_matrix, axis=0)
y_pred_std = np.std(ensemble_matrix, axis=0)
abs_error = np.abs(y_test - y_pred_mean)

# --- 5. Visualization (Parity & Calibration) ---
print("\n--- Plotting Validation Metrics ---")
plt.figure(figsize=(16, 7))

# Plot A: Parity Plot (Accuracy)
# "How close are we to the truth?"
plt.subplot(1, 2, 1)
plt.scatter(y_test, y_pred_mean, alpha=0.6, c='#2b7bba', s=15, edgecolor='k', linewidth=0.3)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2, label='Ideal (y=x)')
plt.xlabel("Actual Heat Capacity ($C_v$)", fontsize=18)
plt.ylabel("Predicted Mean $C_v$", fontsize=18)
plt.title(f"Parity Plot: Accuracy\n$R^2$: {r2_score(y_test, y_pred_mean):.4f}", fontsize=20)
plt.legend(fontsize=16)
plt.grid(True, alpha=0.3)
plt.xticks(fontsize=16)
plt.yticks(fontsize=16)

# Plot B: Uncertainty Calibration (Trustworthiness)
# "Does the model KNOW when it's likely to be wrong?"
correlation, _ = pearsonr(y_pred_std, abs_error)

plt.subplot(1, 2, 2)
sns.regplot(
    x=y_pred_std,
    y=abs_error,
    scatter_kws={'alpha':0.5, 'color':'darkorange', 's':20},
    line_kws={'color':'red', 'lw':2}
)
plt.xlabel("Predicted Uncertainty ($\\sigma_{ensemble}$)", fontsize=20)
plt.ylabel("Absolute Prediction Error ($|y - \\hat{y}|$)", fontsize=20)
plt.title(f"Uncertainty Calibration\nPearson Correlation: {correlation:.3f}", fontsize=18)
plt.grid(True, alpha=0.3)
plt.xticks(fontsize=16)
plt.yticks(fontsize=16)

plt.tight_layout()
plt.show()

# --- 6. Scientific Interpretation ---
print("\n--- üìù Reliability Analysis (For Discussion) ---")
print(f"1. Precision: The ensemble achieved an MAE of {mean_absolute_error(y_test, y_pred_mean):.4f} cal/mol¬∑K.")
print(f"2. Trustworthiness: The correlation between Uncertainty and Error is {correlation:.3f}.")

if correlation > 0.2:
    print("   ‚úÖ PASSED: The model is 'Self-Aware'.")
    print("   It correctly assigns higher uncertainty to molecules it struggles to predict.")
    print("   (This is crucial for safety in chemical engineering deployment).")
else:
    print("   ‚ö†Ô∏è WARNING: Poor Calibration.")
    print("   The model is 'Overconfident'. It makes mistakes without knowing it.")